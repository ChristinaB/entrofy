{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem statement\n",
    "\n",
    "* Input:\n",
    "    - $X \\in \\{0, 1\\}^{n\\times f}$ binary matrix of participants (rows) and their attributes (columns).\n",
    "    - $k < n$ integer number of desired participants.  \n",
    "    - $q \\in [0, 1]^f$ target Bernoulli distribution for each feature\n",
    "    - $w \\in \\mathbb{R}_+^f$ weighting over features\n",
    " \n",
    "* Goal: select a $k$-hot vector $y \\in \\{0, 1\\}^n$ with $\\|y\\| = k$ such that the KL divergence between $D(E[\\langle y, X\\rangle ] ~\\|~ q)$ is minimized.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "### Notes\n",
    "\n",
    "* $D(p~\\|~q) := \\sum_i p_i \\log \\frac{p_i}{q_i}$\n",
    "* This seems np-hard in general, but probably approximable via submodular optimization\n",
    "\n",
    "### Web app\n",
    "\n",
    "* Build this in a nice little flask app with a CSV uploader, and bootstrap widgets to control $k$ and $q$\n",
    "* Visualize the distribution of the solution\n",
    "\n",
    "### Implementation\n",
    "\n",
    "* Let $v_i$ be the indicator vector of selected rows at step $i$ of a greedy algorithm.\n",
    "* The distribution for the $j$th attribute is \n",
    "$$\n",
    "p_j = \\frac{\\langle y, X \\rangle_j}{\\|y\\|}\n",
    "$$\n",
    "\n",
    "* The (weighted) objective function is:\n",
    "\n",
    "$$\n",
    "f(y) = - \\sum_j w_j D(p_j \\| q_j)\n",
    "$$\n",
    "where\n",
    "$$\n",
    "D(a \\| b) = a\\log \\frac{a}{b} + (1-a)\\log\\frac{1-a}{1-b}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We maximize the objective by iteratively optimizing marginal improvement:\n",
    "\n",
    "$$\n",
    "y_{t+1} \\leftarrow y_t + \\text{arg}\\max_e f(y_t + e) - f(y_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p'_j = \\frac{\\langle y, X \\rangle_j + \\langle e, X \\rangle_j }{1 + \\|y\\|}\n",
    "= \\frac{\\langle y, X \\rangle_j + X_{ej} }{1 + \\|y\\|}\n",
    "= \\frac{\\|y\\| p_j + X_{ej}}{1 + \\|y\\|}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Delta f = - \\sum_j w_j \\left(D(p'_j \\| q_j) - D( p_j \\| q_j) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional ideas\n",
    "\n",
    "* Random initialization: run $T$ threads in parallel with random seeds; pick the one that achieves the best objective.\n",
    "* Alternately, initialiaze by the pair of items with the minimum overlap (equivalently, maximum weighted l2 score).\n",
    "* Or just use pre-selects and try to compensate with remaining selectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def obj(p, w, q):\n",
    "    # Prevent numerical underflow in log\n",
    "    \n",
    "    amin = 1e-200\n",
    "    \n",
    "    pbar = 1. - p\n",
    "    qbar = 1. - q\n",
    "    \n",
    "    H = p * (np.log(p + amin) - np.log(q + amin)) + pbar * (np.log(pbar + amin) - np.log(qbar + amin))\n",
    "    return - H.dot(w)\n",
    "\n",
    "def _entrofy(X, k, w=None, q=None, pre_selects=None):\n",
    "    \n",
    "    n_participants, n_attributes = X.shape\n",
    "    \n",
    "    if w is None:\n",
    "        w = np.ones(n_attributes)\n",
    "        \n",
    "    if q is None:\n",
    "        q = 0.5 * np.ones(n_attributes)\n",
    "        \n",
    "    assert 0 < k <= n_participants\n",
    "    assert not np.any(w < 0)\n",
    "    assert np.all(q >= 0.0) and np.all(q <= 1.0)\n",
    "    assert len(w) == n_attributes\n",
    "    assert len(q) == n_attributes\n",
    "    \n",
    "    if k == n_participants:\n",
    "        return np.arange(n_participants)\n",
    "    \n",
    "    # Initialization\n",
    "    y = np.zeros(n_participants, dtype=bool)\n",
    "    \n",
    "    if pre_selects is None:\n",
    "        # Select one at random\n",
    "        pre_selects = np.random.choice(n_participants, size=1)\n",
    "    \n",
    "    y[pre_selects] = True\n",
    "    \n",
    "    while y.sum() < k:\n",
    "        i = y.sum()\n",
    "        # Initialize the distribution vector\n",
    "        p = np.nanmean(X[y], axis=0)\n",
    "        \n",
    "        # Compute the marginal gains\n",
    "        p_new = (p * i + X) / (i + 1.0)\n",
    "        delta = obj(p_new, w, q) - obj(p, w, q)\n",
    "        \n",
    "        # Knock out the points we've already taken\n",
    "        delta[y] = -np.inf\n",
    "        \n",
    "        # Select the top score\n",
    "        y_new = np.argmax(delta)\n",
    "        y[np.argmax(delta)] = True\n",
    "        \n",
    "    return obj(np.nanmean(X[y], axis=0), w, q), np.flatnonzero(y)\n",
    "\n",
    "def entrofy(X, k, w=None, q=None, pre_selects=None, n_samples=15):\n",
    "    '''Entrofy your panel.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray, shape=(n, f), dtype=bool\n",
    "        Rows are participants\n",
    "        Columns are attributes\n",
    "        \n",
    "    k : int in (0, n]\n",
    "        The number of participants to select\n",
    "        \n",
    "    w : np.ndarray, non-negative, shape=(f,)\n",
    "        Weighting over the attributes\n",
    "        By default, a uniform weighting is ued\n",
    "        \n",
    "    q : np.darray, values in [0, 1], shape=(f,)\n",
    "        Target distribution vector for the attributes.\n",
    "        By default, 1/2\n",
    "        \n",
    "    pre_selects : None or iterable\n",
    "        Optionally, you may pre-specify a set of rows to be forced into the solution.\n",
    "        \n",
    "    n_samples : int > 0\n",
    "        If pre_selects is None, run `n_samples` random initializations and return\n",
    "        the solution with the best objective value.\n",
    "        \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    score : float\n",
    "        The score of the solution found.  Larger is better.\n",
    "        \n",
    "    idx : np.ndarray, shape=(k,)\n",
    "        Indicies of the selected rows\n",
    "        \n",
    "    '''\n",
    "    if pre_selects is not None:\n",
    "        n_samples = 1\n",
    "        \n",
    "    results = [_entrofy(X, k, w=w, q=q, pre_selects=pre_selects) for i in range(n_samples)]\n",
    "    \n",
    "    max_score, best = results[0]\n",
    "    for score, solution in results[1:]:\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "            best = solution\n",
    "            \n",
    "    return max_score, best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.random.randn(300, 5)\n",
    "X = np.greater_equal(X, np.linspace(-1.5, 1.5, X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 5)"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score, y = entrofy(X, 25, n_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.010418234187507737,\n",
       " array([  0,   6,   7,  19,  23,  30,  35,  47,  50,  54,  63,  72,  84,\n",
       "         93,  96, 101, 103, 132, 155, 159, 197, 202, 232, 240, 275]))"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.56,  0.52,  0.48,  0.48,  0.48])"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[y].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   6,   7,  19,  23,  30,  35,  47,  50,  54,  63,  72,  84,\n",
       "        93,  96, 101, 103, 132, 155, 159, 197, 202, 232, 240, 275])"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True, False, False],\n",
       "       [ True, False, False, False, False],\n",
       "       [ True,  True, False, False,  True],\n",
       "       [ True,  True,  True, False,  True],\n",
       "       [False, False, False,  True,  True],\n",
       "       [False,  True,  True, False, False],\n",
       "       [ True,  True, False, False,  True],\n",
       "       [False,  True,  True,  True, False],\n",
       "       [ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True, False,  True],\n",
       "       [ True,  True, False,  True,  True],\n",
       "       [False,  True, False,  True, False],\n",
       "       [ True, False,  True, False,  True],\n",
       "       [False, False, False, False, False],\n",
       "       [ True,  True, False,  True,  True],\n",
       "       [ True, False,  True,  True, False],\n",
       "       [ True, False, False, False,  True],\n",
       "       [False, False, False,  True, False],\n",
       "       [ True, False,  True, False,  True],\n",
       "       [ True,  True,  True,  True, False],\n",
       "       [False, False,  True, False, False],\n",
       "       [False, False,  True, False, False],\n",
       "       [False,  True, False,  True, False],\n",
       "       [False, False, False,  True, False],\n",
       "       [False, False, False,  True,  True]], dtype=bool)"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[y]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
